{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My notes of Recurent Neural Network Course\n",
    "coursera https://www.coursera.org/learn/nlp-sequence-models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN (week 1)\n",
    "## Notation\n",
    "Representing words\n",
    "- vocabulary / dictionary \n",
    "  - common size `~30-40k`\n",
    "  - one-hot representation\n",
    "  - token for `<unknown>` words which is not in vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why don't we use standard NN here?\n",
    "- input and output vectors could have different lengths in different examples\n",
    "- doesn't share features learned in different possitions\n",
    "- very large input layer ($10k * max(T_x$))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN\n",
    "\n",
    "input:\n",
    "\n",
    "- $a^{<t-1>}$ and $x^{<t>}$\n",
    "- $a^{<0>}$ usually zero or random values\n",
    "- $a^{<t>}$ come from NN on t-step to the t+1 step\n",
    "\n",
    "output:\n",
    "\n",
    "- $y^{<t>}$\n",
    "\n",
    "calculation:\n",
    "\n",
    "$$\n",
    "a^{<t>} = g_1(W_{aa} a^{<t-1>} + W_{ax} x^{<t>} + b_a)\n",
    "$$\n",
    "$g_1$ - usually tanh/relu\n",
    "\n",
    "$$\n",
    "\\hat{y}^{<t>} = g_2(W_{ya} a^{<t>} + b_y)\n",
    "$$\n",
    "$g_2$ - sigmoid\n",
    "\n",
    "we could collapse for simpler notation\n",
    "\n",
    "$$\n",
    "W_{aa} a^{<t-1>} + W_{ax} x^{<t>} == W_a [ a^{<t-1>} ; x^{<t>} ]\n",
    "$$\n",
    "\n",
    "problems:\n",
    "\n",
    "- we don't have information about upcoming events (words) so sentense which very similar at first part could be completely different on the last part what could influent on result of classifier (possible solution BRNN - bidirectional RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function\n",
    "$$\n",
    "L^{<t>}(\\hat{y}^{<t>}, y^{<t>}) = - y^{<t>} \\log{\\hat{y}^{<t>}} - (1 - y^{<t>}) \\log{(1 - \\hat{y}^{<t>})}\n",
    "$$\n",
    "$$\n",
    "L(\\hat{y}^{<t>}, y^{<t>}) = \\sum_{t=1}^{T_y}{L^{<t>}(\\hat{y}^{<t>}, y^{<t>})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward and Backpropagation through time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "main difference between common is $a^{<t>}$ which pass back and forth \"through time\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Type of RNN. Many-to-Many ($T_x = T_y$)\n",
    "Example: Entity detection in text\n",
    "\n",
    "just ordinar RNN, where "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Type of RNN. Many-to-One\n",
    "output just on last node. Example: Classify movies review,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Type of RNN. One-to-Many\n",
    "input only 1st node and the result just generate\n",
    "Example: Music generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Type of RNN. Many-to-Many ($T_x \\neq T_y$)\n",
    "Example: Translation\n",
    "\n",
    "1st part look like Many-to-One (encoder) and 2nd part as One-to-Many (decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language model and sequence generation\n",
    "- use `RNN` define chance of each word in sentence\n",
    "- get large corpus of text\n",
    "  - `<EOS>` - end of sentence\n",
    "  - `<UNK>` - uknown word\n",
    "- tokenize\n",
    "\n",
    "### the model\n",
    "\n",
    "first input vector is zero $ x^{<0>} = 0 $. Upcoming input vectos are words from real sentence. so we predict change of new word $\\hat{y}^{<t>}$ based on previous one $x^{<t>} = y^{<t - 1>}$ which is sample based on $\\hat{y}^{<t-1>}$ probability. Where $y$ are probabilities of each word from corpus. The last $y^{<t>}$ should be `<EOS>`.\n",
    "\n",
    "Softmax loss function\n",
    "$$\n",
    "L(\\hat{y}^{<t>}, y^{<t>}) = - \\sum_i{y_i^{<t>}\\log{\\hat{y}^{<t>}}}\n",
    "$$\n",
    "$$\n",
    "L = \\sum_t{L(\\hat{y}^{<t>}, y^{<t>})}\n",
    "$$\n",
    "Probability of 3-word sentence\n",
    "$$\n",
    "P(y^{<1>}, y^{<2>}, y^{<3>}) = P(y^{<1>}) P(y^{<2>}|y^{<1>}) P(y^{<3>}|y^{<1>}, y^{<2>})\n",
    "$$\n",
    "Character-level language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems/Weakness. Vanishing gradients with RNNs\n",
    "- classical RNN doesn't very good to catch very long dependecies. in other words classic RNN has local influences\n",
    "- gradient should propagete from all steps of RNN and as result it vanished.\n",
    "- as alternative we could have exploding of gradient - should apply gradient clipping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gated Recurrent Unit (GRU)\n",
    "based on\n",
    "- [2014 On the Properties of Neural Machine Translation: Encoder-Decoder Approaches\n",
    " by Kyunghyun Cho, Bart van Merrienboer, Dzmitry Bahdanau, Yoshua Bengio](https://arxiv.org/abs/1409.1259)\n",
    "- [2014 Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling by Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, Yoshua Bengio](https://arxiv.org/abs/1412.3555)\n",
    "\n",
    "GRU tries solve problem of vanishing gradient and forgetnes useful information for example _singular_ or _plurar_ of one subject.\n",
    "\n",
    "$c$ - memory unit\n",
    "\n",
    "$c^{<t>}$ - state of unit in moment $t$\n",
    "\n",
    "### Simpler version\n",
    "\n",
    "consider overwriting (candidat for replacing $c^{<t>}$)\n",
    "$$\n",
    "\\tilde{c}^{<t>} = tanh(W_c [c^{<t-1>}, x^{<t>}] + b_c)\n",
    "$$ \n",
    "gate (value between [0, 1] and most of the time 0 or 1), u - means 'update'\n",
    "$$\n",
    "\\Gamma_u = \\sigma(W_u [c^{<t-1>}, x^{<t>}] + b_u)\n",
    "$$\n",
    "it decides whether we update $c$ or hold old one\n",
    "$$\n",
    "c^{<t>} = \\Gamma_u \\tilde{c}^{<t>} + (1 - \\Gamma_u) c^{<t-1>}\n",
    "$$\n",
    "output\n",
    "$$\n",
    "y^{<t>} = softmax(c^{<t>})\n",
    "$$\n",
    "\n",
    "and btw in this simpler version $c^{<t>} = a^{<t>}$, memory unit the same as activation value from previous node.\n",
    "\n",
    "### Full version of GRU\n",
    "main difference in those:\n",
    "$$\n",
    "\\tilde{c}^{<t>} = tanh(W_c [\\Gamma_r c^{<t-1>}, x^{<t>}] + b_c)\n",
    "$$\n",
    "$$\n",
    "\\Gamma_r = \\sigma(W_r [c^{<t-1>}, x^{<t>}] + b_r)\n",
    "$$\n",
    "$\\Gamma_r$ means whether we should take into account previous memory unit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long Short Term Memory (LSTM)\n",
    "articles:\n",
    "- [1997 Long Short-term Memory by Sepp Hochreiter and JÃ¼rgen Schmidhuber](https://www.researchgate.net/publication/13853244_Long_Short-term_Memory)\n",
    "\n",
    "$$\n",
    "\\tilde{c}^{<t>} = \\tanh(W_c [\\Gamma_r a^{<t-1>}, x^{<t>}] + b_c)\n",
    "$$\n",
    "update gate\n",
    "$$\n",
    "\\Gamma_u^{<t>} = \\sigma(W_u [a^{<t-1>}, x^{<t>}] + b_u)\n",
    "$$\n",
    "forgetness gate\n",
    "$$\n",
    "\\Gamma_f^{<t>} = \\sigma(W_f [a^{<t-1>}, x^{<t>}] + b_f)\n",
    "$$\n",
    "new memory cell\n",
    "$$\n",
    "c^{<t>} = \\Gamma_f^{<t>} * c^{<t-1>} + \\Gamma_u^{<t>} * \\tilde{c}^{<t>}\n",
    "$$\n",
    "output gate\n",
    "$$\n",
    "\\Gamma_o^{<t>} = \\sigma(W_o [a^{<t-1>}, x^{<t>}] + b_o)\n",
    "$$\n",
    "$$\n",
    "a^{<t>} = \\Gamma_o^{<t>} * \\tanh(c^{<t>})\n",
    "$$\n",
    "output\n",
    "$$\n",
    "y^{<t>} = softmax (a^{<t>})\n",
    "$$\n",
    "\n",
    "### peephole connection\n",
    "sometimes pass $c^{<t-1>}$ with $a^{<t-1>}$ in formulas of $\\Gamma$, [more](https://en.wikipedia.org/wiki/Long_short-term_memory#Peephole_LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM vs GRU\n",
    "- GRU \n",
    "  - is much simpler faster\n",
    "  - we could build much bigger architect and scale\n",
    "- LSTM\n",
    "  - much powerful \n",
    "  - usually default model to try"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional RNN (BRNN)\n",
    "Problem: should know future to correct interpret current state.\n",
    "\n",
    "Solution: We create acyclic graph with backward recurent layer:\n",
    "$$\n",
    "\\overleftarrow{a}^{<t>} \\leftarrow \\overleftarrow{a}^{<t+1>}, x^{<t>}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{y}^{<t>} = g (W_y [ \\overrightarrow{a}^{<t>}, \\overleftarrow{a}^{<t>} ] \n",
    "$$\n",
    "\n",
    "where $\\overrightarrow{a}^{<t>}$ information from pass,\n",
    "and $\\overleftarrow{a}^{<t>}$ information from future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep RNNs\n",
    "- we could stack deep nn on top of $y$ output\n",
    "- it's hard to see a lot of layer of RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP (week 2)\n",
    "## Word representation\n",
    "- vocabrary\n",
    "  - pros\n",
    "    - sparse one-hot vector \n",
    "  - cous\n",
    "    - it doesn't reflect relations between words so it much harder to generalize\n",
    "    - big matrixes\n",
    "- featured representation\n",
    "  - pros\n",
    "    - similar things would likely to have similar vector of features\n",
    "    - reduce side of vector (10k -> 0.3k)\n",
    "  - cons\n",
    "    - smarse vector -> dense vector\n",
    "    - but actually there is no guarantee that we got clear feature because according to linear algebra:\n",
    "    $$\n",
    "    \\theta_i^T e_j = (A \\theta_i)^T (A ^-1 e_j)\n",
    "    $$\n",
    "    we could have any valid $A$ linear degenerate transformation\n",
    "    \n",
    "## Using word embeddings\n",
    "- could easily apply transfer learning \n",
    "  - learn word embedding on large text courpus (1-100b words) \n",
    "  - transfer embedding to new task with smaller training set (100k)\n",
    "    - continue to finetune the word embedding with new data (if you have really big data for traing data set)\n",
    "- cons\n",
    "  - less useful for machine translation (why?)\n",
    "- areas\n",
    "  - entity recognition\n",
    "  - text summarization\n",
    "  - co-reference resolution\n",
    "  - parsing\n",
    "  - etc\n",
    "- less useful (where you have big traing data set in our area - 2nd)\n",
    "  - language modeling\n",
    "  - machine translation\n",
    "- related to:\n",
    "  - face encoding (represent face in small vector)\n",
    "    - difference: \n",
    "      - goal of face - encode any face\n",
    "      - goal of nlp - represent relativy fix dictionary but highlight relation and differences between entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Properties of word embeddings\n",
    "$$\n",
    "e_{man} - e_{woman} \\approx e_{king} - e_w\n",
    "$$\n",
    "\n",
    "$$\n",
    "argmax_w( sim(e_w, e_{king} - e_{man} + e_{woman}))\n",
    "$$\n",
    "\n",
    "what usually gives 30-70% accuracy, according to sci papers\n",
    "\n",
    "- `sim` - usually is consine similarty\n",
    "$$\n",
    "sim = \\frac{u^Tv}{||u|| ||v||}\n",
    "$$\n",
    "if u and v very similar inner product ($u^Tv$) will be very large.\n",
    "\n",
    "It's named this way because it is `cos` between `u` and `v`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Learning word embedding\n",
    "- n-gramms good for language modeling\n",
    "- but for word embedding ever 1 last word, nearby 1 word or skip-gram are enough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Word2Vec\n",
    "[Efficient Estimation of Word Representations in Vector Space - Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean](https://arxiv.org/abs/1301.3781)\n",
    "- it based on skip-gram model\n",
    "- get one `context` word for the `target`\n",
    "- model\n",
    "$$O_c E = e_c$$\n",
    "$E$ - embedding matrix\n",
    "$e_c$ - embedding vector\n",
    "\n",
    "$e_c$ -> softmax -> $\\hat{y}$\n",
    "\n",
    "$\\hat{y} = p(t|c)$ - probability of target word t in context of c\n",
    "\n",
    "$$\n",
    "p(t|c) = \\frac{e^{\\theta_t^T e_c}}{\\sum_{j=1}^{10k}{e^{\\theta_j^T e_c}}}\n",
    "$$\n",
    "\n",
    "$\\theta$ - paramtere associated with output t.\n",
    "- loss function\n",
    "$$\n",
    "L(\\hat{y},y) = - \\sum_{i=1}^{10k}{y_i \\log{\\hat{y_i}}}\n",
    "$$\n",
    "\n",
    "$y_i$ - one-hot vector which have 0 everywhere and only single 1 for the word\n",
    "- $p(c)$ we could take into account that some words are more popular then another\n",
    "### primary problem\n",
    "computation speed - too many parameters to evaluate probability $p(t|c)$ (softmax)\n",
    "### workaround\n",
    "hierarchical softmax. \n",
    "- reduce number of calculation from N to log N\n",
    "- it is not ballanced tree but more popular words lay on lover levels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative sampling\n",
    "[Distributed Representations of Words and Phrases and their Compositionality - Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, Jeffrey Dean](https://arxiv.org/abs/1310.4546)\n",
    "- get single context and single target word - it will be possitive sample\n",
    "- get k random words from dictionary - they will be negative samples\n",
    "- k from [5,20] for smaller dataset\n",
    "- k from [2,5] for large dataset\n",
    "- model\n",
    "$$\n",
    "P(y=1 | c,t) = \\sigma(\\theta_t^T e_c)\n",
    "$$\n",
    "- instead of classifing 10k (size of dictionary) classes, we will have 10k binarry classifers\n",
    "- empirical distribution for sampling\n",
    "$$\n",
    "P(w_i) = \\frac{f(w_i)^{3/4}}{\\sum_{j=1}^{10k}{f(w_j)^{3/4}}}\n",
    "$$\n",
    "where $f(w_j)$ is obsorve frequency of word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe word vectos\n",
    "[GloVe: Global Vectors for Word Representation Jeffrey Pennington,   Richard Socher,   Christopher D. Manning](https://nlp.stanford.edu/projects/glove/)\n",
    "- model\n",
    "$X_{ij}$ - how much $j$ in context of $i$, $X_{ij} = X_{ji}$\n",
    "minimize\n",
    "$$\n",
    "minimize = \\sum_{i=1}^{10k}\\sum_{j=1}^{10k}f(X_{ij})(\\theta_i^T e_j + b_i + b_j - \\log{X_{ij}})^2\n",
    "$$\n",
    "$f(X_{ij})$ - heuristic, weighting term which $= 0$ when $X_{ij} = 0$ to \"compensate\" $\\log 0$ (here convension $0 \\log 0 = 0$,\n",
    "also it give more weight for less frequent words\n",
    "\n",
    "$$\n",
    "e_{final} = \\frac{e_{wt}\\theta_w}{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application. Sentiment classification\n",
    "word embedding could help in case of small dataset.\n",
    "- ? (For me it looks like bag of words)\n",
    "  - model\n",
    "    - get all words from sentence\n",
    "    - calculate embedding vector for them\n",
    "    - avg or sum them \n",
    "    - apply softmax to get probability of each classes\n",
    "  - problem\n",
    "    - ignore words order (for example \"not good\" and \"good\" here would be almost the same)\n",
    "- many-to-one RNN architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application. Debiasing word embeddings\n",
    "[Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings Tolga Bolukbasi, Kai-Wei Chang, James Zou, Venkatesh Saligrama, Adam Kalai](https://arxiv.org/abs/1607.06520)\n",
    "- example: \n",
    "  - _Man -> Computer_Programmer as Woman -> Homemaker_\n",
    "  - _Father -> Doctor as Mother as Nurse_\n",
    "- word embeddings can reflect gender, ethnicity, age, sexual orientation and etc\n",
    "- solution\n",
    "  - identify bies direction:\n",
    "  $$\n",
    "  e_{hi} - e_{she}\n",
    "  $$\n",
    "  $$\n",
    "  e_{male} - e_{female}\n",
    "  $$\n",
    "  and etc and avg them.\n",
    "  - neutralize - for every word that is not definitional, project to get rid of bies (for example: `doctor`, `babysitter`, etc)\n",
    "  - equalize pairs (`grandmother <-> grandfather`, `girl <-> boy`, etc) there shouldn't be difference between pair words except gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
